{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import figure\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import TimeSeriesSplit, train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References:\n",
    "1. https://machinelearningmastery.com/how-to-develop-lstm-models-for-time-series-forecasting/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Settings:\n",
    "pd.set_option('display.width', 190)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 20)\n",
    "pd.set_option('max_colwidth', 200)\n",
    "pd.options.display.float_format = '{:.4f}'.format\n",
    "plt.style.use('default')\n",
    "np.set_printoptions(threshold = 30, edgeitems = 30, precision = 2, suppress = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_sequences(Xs, ys, n_steps):\n",
    "    X, y = list(), list()\n",
    "    for i in range(len(ys)):\n",
    "        # find the end of this pattern\n",
    "        end_ix = i + n_steps\n",
    "        # check if we are beyond the dataset\n",
    "        if end_ix > len(ys):\n",
    "            break\n",
    "        # gather input and output parts of the pattern\n",
    "        seq_x, seq_y = Xs[i: end_ix], ys[end_ix - 1]\n",
    "        X.append(seq_x)\n",
    "        y.append(seq_y)\n",
    "    return np.array(X), np.array(y).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "from skorch import NeuralNetBinaryClassifier\n",
    "\n",
    "\n",
    "class MyModule(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyModule, self).__init__()\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size=15, hidden_size=6, num_layers=1, batch_first=True)\n",
    "        self.dense = nn.Linear(6, 1)\n",
    "\n",
    "    def forward(self, X, **kwargs):\n",
    "        output, hidden = self.lstm(X)\n",
    "        X = self.dense(output[:, -1, :])\n",
    "        return X\n",
    "\n",
    "def get_model():\n",
    "    model = NeuralNetBinaryClassifier(\n",
    "        MyModule,\n",
    "        optimizer=Adam,\n",
    "        max_epochs=50,\n",
    "        lr=3e-4,\n",
    "        batch_size=16,\n",
    "        iterator_train__shuffle=True,\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_path = \"../merged_data/features_USRECD.csv\"\n",
    "features = [\"BCI\", \"BCIp\", \"BCIg\", 'IE_SP_Comp', 'IE_SP_Dividend', 'IE_SP_Earnings', 'IE_Consumer_CPI', 'IE_Long_Interest', 'IE_Real_Price', 'IE_Real_Dividend', 'IE_Return_Price', 'IE_Real_Earnings',\n",
    "                'IE_Scaled_Earnings', 'IE_Monthly_Returns', 'IE_Real_Returns']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BCI</th>\n",
       "      <th>BCIp</th>\n",
       "      <th>BCIg</th>\n",
       "      <th>USRECD</th>\n",
       "      <th>IE_SP_Comp</th>\n",
       "      <th>IE_SP_Dividend</th>\n",
       "      <th>IE_SP_Earnings</th>\n",
       "      <th>IE_Consumer_CPI</th>\n",
       "      <th>IE_Long_Interest</th>\n",
       "      <th>IE_Real_Price</th>\n",
       "      <th>IE_Real_Dividend</th>\n",
       "      <th>IE_Return_Price</th>\n",
       "      <th>IE_Real_Earnings</th>\n",
       "      <th>IE_Scaled_Earnings</th>\n",
       "      <th>IE_Monthly_Returns</th>\n",
       "      <th>IE_Real_Returns</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1967-02-09</th>\n",
       "      <td>4.6052</td>\n",
       "      <td>6.5870</td>\n",
       "      <td>3.4751</td>\n",
       "      <td>0</td>\n",
       "      <td>4.4362</td>\n",
       "      <td>1.0578</td>\n",
       "      <td>1.7084</td>\n",
       "      <td>3.4935</td>\n",
       "      <td>1.5217</td>\n",
       "      <td>6.5522</td>\n",
       "      <td>3.1739</td>\n",
       "      <td>11.4502</td>\n",
       "      <td>3.8238</td>\n",
       "      <td>8.7218</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>2.5153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1967-02-16</th>\n",
       "      <td>4.6052</td>\n",
       "      <td>6.5863</td>\n",
       "      <td>3.4751</td>\n",
       "      <td>0</td>\n",
       "      <td>4.4362</td>\n",
       "      <td>1.0578</td>\n",
       "      <td>1.7084</td>\n",
       "      <td>3.4935</td>\n",
       "      <td>1.5217</td>\n",
       "      <td>6.5522</td>\n",
       "      <td>3.1739</td>\n",
       "      <td>11.4502</td>\n",
       "      <td>3.8238</td>\n",
       "      <td>8.7218</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>2.5153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1967-02-23</th>\n",
       "      <td>4.6012</td>\n",
       "      <td>6.5774</td>\n",
       "      <td>3.4751</td>\n",
       "      <td>0</td>\n",
       "      <td>4.4362</td>\n",
       "      <td>1.0578</td>\n",
       "      <td>1.7084</td>\n",
       "      <td>3.4935</td>\n",
       "      <td>1.5217</td>\n",
       "      <td>6.5522</td>\n",
       "      <td>3.1739</td>\n",
       "      <td>11.4502</td>\n",
       "      <td>3.8238</td>\n",
       "      <td>8.7218</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>2.5153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1967-03-02</th>\n",
       "      <td>4.6032</td>\n",
       "      <td>6.5820</td>\n",
       "      <td>3.4751</td>\n",
       "      <td>0</td>\n",
       "      <td>4.4700</td>\n",
       "      <td>1.0613</td>\n",
       "      <td>1.7011</td>\n",
       "      <td>3.4935</td>\n",
       "      <td>1.5326</td>\n",
       "      <td>6.5860</td>\n",
       "      <td>3.1772</td>\n",
       "      <td>11.4869</td>\n",
       "      <td>3.8177</td>\n",
       "      <td>8.7185</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>2.5153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1967-03-09</th>\n",
       "      <td>4.6042</td>\n",
       "      <td>6.5852</td>\n",
       "      <td>3.4751</td>\n",
       "      <td>0</td>\n",
       "      <td>4.4700</td>\n",
       "      <td>1.0613</td>\n",
       "      <td>1.7011</td>\n",
       "      <td>3.4935</td>\n",
       "      <td>1.5326</td>\n",
       "      <td>6.5860</td>\n",
       "      <td>3.1772</td>\n",
       "      <td>11.4869</td>\n",
       "      <td>3.8177</td>\n",
       "      <td>8.7185</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>2.5153</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              BCI   BCIp   BCIg  USRECD  IE_SP_Comp  IE_SP_Dividend  IE_SP_Earnings  IE_Consumer_CPI  IE_Long_Interest  IE_Real_Price  IE_Real_Dividend  IE_Return_Price  IE_Real_Earnings  \\\n",
       "Date                                                                                                                                                                                         \n",
       "1967-02-09 4.6052 6.5870 3.4751       0      4.4362          1.0578          1.7084           3.4935            1.5217         6.5522            3.1739          11.4502            3.8238   \n",
       "1967-02-16 4.6052 6.5863 3.4751       0      4.4362          1.0578          1.7084           3.4935            1.5217         6.5522            3.1739          11.4502            3.8238   \n",
       "1967-02-23 4.6012 6.5774 3.4751       0      4.4362          1.0578          1.7084           3.4935            1.5217         6.5522            3.1739          11.4502            3.8238   \n",
       "1967-03-02 4.6032 6.5820 3.4751       0      4.4700          1.0613          1.7011           3.4935            1.5326         6.5860            3.1772          11.4869            3.8177   \n",
       "1967-03-09 4.6042 6.5852 3.4751       0      4.4700          1.0613          1.7011           3.4935            1.5326         6.5860            3.1772          11.4869            3.8177   \n",
       "\n",
       "            IE_Scaled_Earnings  IE_Monthly_Returns  IE_Real_Returns  \n",
       "Date                                                                 \n",
       "1967-02-09              8.7218              0.0000           2.5153  \n",
       "1967-02-16              8.7218              0.0000           2.5153  \n",
       "1967-02-23              8.7218              0.0000           2.5153  \n",
       "1967-03-02              8.7185              0.0100           2.5153  \n",
       "1967-03-09              8.7185              0.0100           2.5153  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the data and do a little bit of wrangling:\n",
    "df = pd.read_csv(df_path)\n",
    "df.Date = pd.to_datetime(df.Date)\n",
    "df = df.set_index(\"Date\", drop=True)\n",
    "df = df.drop(columns=\"Unnamed: 0\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into training and test sets and hold out the test set until the end, so that it remains \"unseen\".\n",
    "lag_of_y = 21 # This is the lag we introduce to the target variable so that we assess the indicator's \n",
    "              # ability to predict the target variable this many steps into the future.\n",
    "              # With BCI, a lag of 21 data points corresponds to about half a year.\n",
    "        \n",
    "X_train, X_test, y_train, y_test = train_test_split(df.iloc[:-lag_of_y, df.columns != \"USRECD\"], \\\n",
    "    df.iloc[lag_of_y:, df.columns == \"USRECD\"], test_size=0.1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
      "  18  19  20  21  22  23  24  25  26  27  28  29 ... 471 472 473 474 475\n",
      " 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493\n",
      " 494 495 496 497 498 499 500] TEST: [ 501  502  503  504  505  506  507  508  509  510  511  512  513  514\n",
      "  515  516  517  518  519  520  521  522  523  524  525  526  527  528\n",
      "  529  530 ...  971  972  973  974  975  976  977  978  979  980  981  982\n",
      "  983  984  985  986  987  988  989  990  991  992  993  994  995  996\n",
      "  997  998  999 1000]\n",
      "TRAIN: [   0    1    2    3    4    5    6    7    8    9   10   11   12   13\n",
      "   14   15   16   17   18   19   20   21   22   23   24   25   26   27\n",
      "   28   29 ...  971  972  973  974  975  976  977  978  979  980  981  982\n",
      "  983  984  985  986  987  988  989  990  991  992  993  994  995  996\n",
      "  997  998  999 1000] TEST: [1001 1002 1003 1004 1005 1006 1007 1008 1009 1010 1011 1012 1013 1014\n",
      " 1015 1016 1017 1018 1019 1020 1021 1022 1023 1024 1025 1026 1027 1028\n",
      " 1029 1030 ... 1471 1472 1473 1474 1475 1476 1477 1478 1479 1480 1481 1482\n",
      " 1483 1484 1485 1486 1487 1488 1489 1490 1491 1492 1493 1494 1495 1496\n",
      " 1497 1498 1499 1500]\n",
      "TRAIN: [   0    1    2    3    4    5    6    7    8    9   10   11   12   13\n",
      "   14   15   16   17   18   19   20   21   22   23   24   25   26   27\n",
      "   28   29 ... 1471 1472 1473 1474 1475 1476 1477 1478 1479 1480 1481 1482\n",
      " 1483 1484 1485 1486 1487 1488 1489 1490 1491 1492 1493 1494 1495 1496\n",
      " 1497 1498 1499 1500] TEST: [1501 1502 1503 1504 1505 1506 1507 1508 1509 1510 1511 1512 1513 1514\n",
      " 1515 1516 1517 1518 1519 1520 1521 1522 1523 1524 1525 1526 1527 1528\n",
      " 1529 1530 ... 1971 1972 1973 1974 1975 1976 1977 1978 1979 1980 1981 1982\n",
      " 1983 1984 1985 1986 1987 1988 1989 1990 1991 1992 1993 1994 1995 1996\n",
      " 1997 1998 1999 2000]\n",
      "TRAIN: [   0    1    2    3    4    5    6    7    8    9   10   11   12   13\n",
      "   14   15   16   17   18   19   20   21   22   23   24   25   26   27\n",
      "   28   29 ... 1971 1972 1973 1974 1975 1976 1977 1978 1979 1980 1981 1982\n",
      " 1983 1984 1985 1986 1987 1988 1989 1990 1991 1992 1993 1994 1995 1996\n",
      " 1997 1998 1999 2000] TEST: [2001 2002 2003 2004 2005 2006 2007 2008 2009 2010 2011 2012 2013 2014\n",
      " 2015 2016 2017 2018 2019 2020 2021 2022 2023 2024 2025 2026 2027 2028\n",
      " 2029 2030 ... 2471 2472 2473 2474 2475 2476 2477 2478 2479 2480 2481 2482\n",
      " 2483 2484 2485 2486 2487 2488 2489 2490 2491 2492 2493 2494 2495 2496\n",
      " 2497 2498 2499 2500]\n"
     ]
    }
   ],
   "source": [
    "# Do a time series cross-validation on the test set by splitting it to k folds and doing a \"rolling\"\n",
    "# validation against a validation fold, then averaging out the metrics.\n",
    "splits = 4 # This is the number of splits/folds in the rolling validation.\n",
    "tscv = TimeSeriesSplit(n_splits=splits)\n",
    "\n",
    "for train_index, test_index in tscv.split(X_train): # Rolling cross-validation happens inside this loop.\n",
    "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m0.6374\u001b[0m       \u001b[32m0.7474\u001b[0m        \u001b[35m0.6683\u001b[0m  0.0251\n",
      "      2        \u001b[36m0.6248\u001b[0m       0.7474        \u001b[35m0.6651\u001b[0m  0.0249\n",
      "      3        \u001b[36m0.6132\u001b[0m       0.7474        \u001b[35m0.6606\u001b[0m  0.0243\n",
      "      4        \u001b[36m0.6019\u001b[0m       0.7474        \u001b[35m0.6558\u001b[0m  0.0239\n",
      "      5        \u001b[36m0.5912\u001b[0m       0.7474        \u001b[35m0.6510\u001b[0m  0.0236\n",
      "      6        \u001b[36m0.5803\u001b[0m       0.7474        \u001b[35m0.6471\u001b[0m  0.0236\n",
      "      7        \u001b[36m0.5693\u001b[0m       0.7474        \u001b[35m0.6437\u001b[0m  0.0241\n",
      "      8        \u001b[36m0.5573\u001b[0m       0.7474        \u001b[35m0.6405\u001b[0m  0.0236\n",
      "      9        \u001b[36m0.5447\u001b[0m       0.7474        \u001b[35m0.6378\u001b[0m  0.0242\n",
      "     10        \u001b[36m0.5317\u001b[0m       0.7474        \u001b[35m0.6349\u001b[0m  0.0235\n",
      "     11        \u001b[36m0.5177\u001b[0m       0.7474        \u001b[35m0.6318\u001b[0m  0.0241\n",
      "     12        \u001b[36m0.5032\u001b[0m       0.7474        \u001b[35m0.6291\u001b[0m  0.0236\n",
      "     13        \u001b[36m0.4883\u001b[0m       0.7474        \u001b[35m0.6254\u001b[0m  0.0238\n",
      "     14        \u001b[36m0.4731\u001b[0m       0.7474        \u001b[35m0.6214\u001b[0m  0.0234\n",
      "     15        \u001b[36m0.4582\u001b[0m       0.7474        \u001b[35m0.6174\u001b[0m  0.0235\n",
      "     16        \u001b[36m0.4437\u001b[0m       0.7474        \u001b[35m0.6133\u001b[0m  0.0234\n",
      "     17        \u001b[36m0.4298\u001b[0m       0.7474        \u001b[35m0.6091\u001b[0m  0.0238\n",
      "     18        \u001b[36m0.4164\u001b[0m       0.7474        \u001b[35m0.6056\u001b[0m  0.0233\n",
      "     19        \u001b[36m0.4034\u001b[0m       0.7474        \u001b[35m0.6011\u001b[0m  0.0238\n",
      "     20        \u001b[36m0.3911\u001b[0m       0.7474        \u001b[35m0.5960\u001b[0m  0.0236\n",
      "     21        \u001b[36m0.3794\u001b[0m       0.7474        \u001b[35m0.5924\u001b[0m  0.0239\n",
      "     22        \u001b[36m0.3681\u001b[0m       0.7474        \u001b[35m0.5879\u001b[0m  0.0237\n",
      "     23        \u001b[36m0.3571\u001b[0m       0.7474        \u001b[35m0.5840\u001b[0m  0.0239\n",
      "     24        \u001b[36m0.3463\u001b[0m       0.7474        \u001b[35m0.5794\u001b[0m  0.0235\n",
      "     25        \u001b[36m0.3362\u001b[0m       \u001b[32m0.7579\u001b[0m        \u001b[35m0.5752\u001b[0m  0.0238\n",
      "     26        \u001b[36m0.3260\u001b[0m       \u001b[32m0.7684\u001b[0m        \u001b[35m0.5705\u001b[0m  0.0235\n",
      "     27        \u001b[36m0.3161\u001b[0m       \u001b[32m0.7789\u001b[0m        \u001b[35m0.5660\u001b[0m  0.0238\n",
      "     28        \u001b[36m0.3067\u001b[0m       \u001b[32m0.7895\u001b[0m        \u001b[35m0.5606\u001b[0m  0.0232\n",
      "     29        \u001b[36m0.2970\u001b[0m       \u001b[32m0.8000\u001b[0m        \u001b[35m0.5578\u001b[0m  0.0238\n",
      "     30        \u001b[36m0.2876\u001b[0m       \u001b[32m0.8211\u001b[0m        \u001b[35m0.5522\u001b[0m  0.0236\n",
      "     31        \u001b[36m0.2783\u001b[0m       \u001b[32m0.8316\u001b[0m        \u001b[35m0.5478\u001b[0m  0.0242\n",
      "     32        \u001b[36m0.2689\u001b[0m       0.8316        \u001b[35m0.5436\u001b[0m  0.0244\n",
      "     33        \u001b[36m0.2601\u001b[0m       0.8316        \u001b[35m0.5385\u001b[0m  0.0239\n",
      "     34        \u001b[36m0.2509\u001b[0m       0.8316        \u001b[35m0.5333\u001b[0m  0.0243\n",
      "     35        \u001b[36m0.2431\u001b[0m       0.8316        \u001b[35m0.5290\u001b[0m  0.0240\n",
      "     36        \u001b[36m0.2338\u001b[0m       0.8316        \u001b[35m0.5249\u001b[0m  0.0238\n",
      "     37        \u001b[36m0.2258\u001b[0m       0.8316        \u001b[35m0.5205\u001b[0m  0.0239\n",
      "     38        \u001b[36m0.2180\u001b[0m       \u001b[32m0.8421\u001b[0m        \u001b[35m0.5150\u001b[0m  0.0245\n",
      "     39        \u001b[36m0.2107\u001b[0m       0.8421        \u001b[35m0.5103\u001b[0m  0.0239\n",
      "     40        \u001b[36m0.2046\u001b[0m       0.8421        \u001b[35m0.5039\u001b[0m  0.0233\n",
      "     41        \u001b[36m0.1991\u001b[0m       0.8421        \u001b[35m0.4986\u001b[0m  0.0233\n",
      "     42        \u001b[36m0.1923\u001b[0m       0.8421        \u001b[35m0.4946\u001b[0m  0.0235\n",
      "     43        \u001b[36m0.1854\u001b[0m       0.8421        \u001b[35m0.4917\u001b[0m  0.0236\n",
      "     44        \u001b[36m0.1790\u001b[0m       0.8421        \u001b[35m0.4840\u001b[0m  0.0234\n",
      "     45        \u001b[36m0.1751\u001b[0m       \u001b[32m0.8526\u001b[0m        \u001b[35m0.4793\u001b[0m  0.0236\n",
      "     46        \u001b[36m0.1678\u001b[0m       0.8526        \u001b[35m0.4743\u001b[0m  0.0233\n",
      "     47        \u001b[36m0.1639\u001b[0m       0.8526        \u001b[35m0.4701\u001b[0m  0.0242\n",
      "     48        \u001b[36m0.1584\u001b[0m       0.8526        \u001b[35m0.4651\u001b[0m  0.0233\n",
      "     49        \u001b[36m0.1530\u001b[0m       0.8526        \u001b[35m0.4603\u001b[0m  0.0237\n",
      "     50        \u001b[36m0.1492\u001b[0m       0.8526        \u001b[35m0.4571\u001b[0m  0.0235\n",
      "0.5467228070175438 0.7978723404255319\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m0.7329\u001b[0m       \u001b[32m0.2974\u001b[0m        \u001b[35m0.7456\u001b[0m  0.0475\n",
      "      2        \u001b[36m0.7097\u001b[0m       \u001b[32m0.3077\u001b[0m        \u001b[35m0.7384\u001b[0m  0.0472\n",
      "      3        \u001b[36m0.6842\u001b[0m       \u001b[32m0.3487\u001b[0m        \u001b[35m0.7300\u001b[0m  0.0473\n",
      "      4        \u001b[36m0.6562\u001b[0m       \u001b[32m0.4256\u001b[0m        \u001b[35m0.7173\u001b[0m  0.0473\n",
      "      5        \u001b[36m0.6271\u001b[0m       \u001b[32m0.5128\u001b[0m        \u001b[35m0.7004\u001b[0m  0.0473\n",
      "      6        \u001b[36m0.5986\u001b[0m       \u001b[32m0.5590\u001b[0m        \u001b[35m0.6872\u001b[0m  0.0472\n",
      "      7        \u001b[36m0.5714\u001b[0m       \u001b[32m0.5897\u001b[0m        \u001b[35m0.6789\u001b[0m  0.0471\n",
      "      8        \u001b[36m0.5454\u001b[0m       0.5744        \u001b[35m0.6770\u001b[0m  0.0472\n",
      "      9        \u001b[36m0.5209\u001b[0m       0.5487        \u001b[35m0.6770\u001b[0m  0.0473\n",
      "     10        \u001b[36m0.4979\u001b[0m       0.4513        0.6869  0.0475\n",
      "     11        \u001b[36m0.4756\u001b[0m       0.4513        0.6921  0.0469\n",
      "     12        \u001b[36m0.4541\u001b[0m       0.4513        0.7040  0.0471\n",
      "     13        \u001b[36m0.4316\u001b[0m       0.4513        0.7093  0.0476\n",
      "     14        \u001b[36m0.4081\u001b[0m       0.4462        0.7154  0.0473\n",
      "     15        \u001b[36m0.3811\u001b[0m       0.4462        0.7255  0.0472\n",
      "     16        \u001b[36m0.3571\u001b[0m       0.4410        0.7545  0.0471\n",
      "     17        \u001b[36m0.3364\u001b[0m       0.4359        0.7665  0.0471\n",
      "     18        \u001b[36m0.3174\u001b[0m       0.4359        0.7702  0.0471\n",
      "     19        \u001b[36m0.3006\u001b[0m       0.4359        0.7700  0.0473\n",
      "     20        \u001b[36m0.2855\u001b[0m       0.4308        0.7743  0.0470\n",
      "     21        \u001b[36m0.2720\u001b[0m       0.4308        0.7786  0.0472\n",
      "     22        \u001b[36m0.2592\u001b[0m       0.4308        0.7849  0.0473\n",
      "     23        \u001b[36m0.2473\u001b[0m       0.4308        0.7870  0.0473\n",
      "     24        \u001b[36m0.2364\u001b[0m       0.4308        0.7955  0.0470\n",
      "     25        \u001b[36m0.2249\u001b[0m       0.4308        0.7980  0.0470\n",
      "     26        \u001b[36m0.2139\u001b[0m       0.4308        0.8046  0.0475\n",
      "     27        \u001b[36m0.2049\u001b[0m       0.4308        0.8084  0.0474\n",
      "     28        \u001b[36m0.1969\u001b[0m       0.4256        0.8202  0.0485\n",
      "     29        \u001b[36m0.1882\u001b[0m       0.4256        0.8226  0.0495\n",
      "     30        \u001b[36m0.1832\u001b[0m       0.4256        0.8259  0.0472\n",
      "     31        \u001b[36m0.1753\u001b[0m       0.4256        0.8349  0.0475\n",
      "     32        \u001b[36m0.1709\u001b[0m       0.4256        0.8436  0.0472\n",
      "     33        \u001b[36m0.1612\u001b[0m       0.4256        0.8460  0.0472\n",
      "     34        \u001b[36m0.1569\u001b[0m       0.4256        0.8518  0.0476\n",
      "     35        \u001b[36m0.1493\u001b[0m       0.4256        0.8568  0.0475\n",
      "     36        \u001b[36m0.1418\u001b[0m       0.4256        0.8612  0.0474\n",
      "     37        \u001b[36m0.1346\u001b[0m       0.4256        0.8636  0.0479\n",
      "     38        0.1347       0.4256        0.8706  0.0472\n",
      "     39        \u001b[36m0.1283\u001b[0m       0.4256        0.8774  0.0473\n",
      "     40        \u001b[36m0.1253\u001b[0m       0.4256        0.8845  0.0473\n",
      "     41        \u001b[36m0.1181\u001b[0m       0.4256        0.8842  0.0475\n",
      "     42        \u001b[36m0.1133\u001b[0m       0.4256        0.8930  0.0473\n",
      "     43        \u001b[36m0.1096\u001b[0m       0.4256        0.8959  0.0473\n",
      "     44        \u001b[36m0.1078\u001b[0m       0.4256        0.9038  0.0472\n",
      "     45        \u001b[36m0.1030\u001b[0m       0.4256        0.9096  0.0473\n",
      "     46        \u001b[36m0.1025\u001b[0m       0.4256        0.9128  0.0474\n",
      "     47        \u001b[36m0.1001\u001b[0m       0.4256        0.9240  0.0478\n",
      "     48        \u001b[36m0.0961\u001b[0m       0.4256        0.9283  0.0474\n",
      "     49        \u001b[36m0.0933\u001b[0m       0.4256        0.9336  0.0475\n",
      "     50        \u001b[36m0.0915\u001b[0m       0.4256        0.9445  0.0472\n",
      "0.38620689655172413 0.07446808510638298\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m0.6168\u001b[0m       \u001b[32m0.8305\u001b[0m        \u001b[35m0.5802\u001b[0m  0.0716\n",
      "      2        \u001b[36m0.5963\u001b[0m       0.8305        \u001b[35m0.5762\u001b[0m  0.0709\n",
      "      3        \u001b[36m0.5757\u001b[0m       0.8305        \u001b[35m0.5730\u001b[0m  0.0711\n",
      "      4        \u001b[36m0.5517\u001b[0m       0.8305        \u001b[35m0.5710\u001b[0m  0.0712\n",
      "      5        \u001b[36m0.5233\u001b[0m       0.8305        \u001b[35m0.5692\u001b[0m  0.0709\n",
      "      6        \u001b[36m0.4926\u001b[0m       0.8305        0.5708  0.0709\n",
      "      7        \u001b[36m0.4602\u001b[0m       0.8305        0.5741  0.0709\n",
      "      8        \u001b[36m0.4299\u001b[0m       \u001b[32m0.9085\u001b[0m        0.5766  0.0720\n",
      "      9        \u001b[36m0.4037\u001b[0m       0.8542        0.5783  0.0709\n",
      "     10        \u001b[36m0.3810\u001b[0m       0.7627        0.5802  0.0734\n",
      "     11        \u001b[36m0.3618\u001b[0m       0.7322        0.5784  0.0715\n",
      "     12        \u001b[36m0.3446\u001b[0m       0.6712        0.5780  0.0713\n",
      "     13        \u001b[36m0.3292\u001b[0m       0.6610        0.5765  0.0705\n",
      "     14        \u001b[36m0.3147\u001b[0m       0.6542        0.5746  0.0708\n",
      "     15        \u001b[36m0.3009\u001b[0m       0.6441        0.5717  0.0713\n",
      "     16        \u001b[36m0.2879\u001b[0m       0.6508        \u001b[35m0.5683\u001b[0m  0.0707\n",
      "     17        \u001b[36m0.2757\u001b[0m       0.6508        \u001b[35m0.5659\u001b[0m  0.0709\n",
      "     18        \u001b[36m0.2647\u001b[0m       0.6508        \u001b[35m0.5626\u001b[0m  0.0714\n",
      "     19        \u001b[36m0.2545\u001b[0m       0.6508        \u001b[35m0.5600\u001b[0m  0.0708\n",
      "     20        \u001b[36m0.2457\u001b[0m       0.6508        \u001b[35m0.5587\u001b[0m  0.0707\n",
      "     21        \u001b[36m0.2364\u001b[0m       0.6508        \u001b[35m0.5551\u001b[0m  0.0709\n",
      "     22        \u001b[36m0.2288\u001b[0m       0.6508        \u001b[35m0.5521\u001b[0m  0.0710\n",
      "     23        \u001b[36m0.2210\u001b[0m       0.6542        \u001b[35m0.5511\u001b[0m  0.0704\n",
      "     24        \u001b[36m0.2126\u001b[0m       0.6542        \u001b[35m0.5462\u001b[0m  0.0707\n",
      "     25        \u001b[36m0.2079\u001b[0m       0.6508        \u001b[35m0.5460\u001b[0m  0.0704\n",
      "     26        \u001b[36m0.2003\u001b[0m       0.6508        \u001b[35m0.5445\u001b[0m  0.0709\n",
      "     27        \u001b[36m0.1950\u001b[0m       0.6542        \u001b[35m0.5394\u001b[0m  0.0713\n",
      "     28        \u001b[36m0.1889\u001b[0m       0.6508        0.5427  0.0716\n",
      "     29        \u001b[36m0.1841\u001b[0m       0.6475        \u001b[35m0.5392\u001b[0m  0.0708\n",
      "     30        \u001b[36m0.1797\u001b[0m       0.6475        \u001b[35m0.5331\u001b[0m  0.0703\n",
      "     31        \u001b[36m0.1748\u001b[0m       0.6407        \u001b[35m0.5323\u001b[0m  0.0708\n",
      "     32        \u001b[36m0.1712\u001b[0m       0.6305        0.5342  0.0709\n",
      "     33        \u001b[36m0.1657\u001b[0m       0.6339        \u001b[35m0.5314\u001b[0m  0.0714\n",
      "     34        \u001b[36m0.1624\u001b[0m       0.6305        \u001b[35m0.5277\u001b[0m  0.0713\n",
      "     35        \u001b[36m0.1573\u001b[0m       0.6305        \u001b[35m0.5258\u001b[0m  0.0718\n",
      "     36        \u001b[36m0.1534\u001b[0m       0.6271        \u001b[35m0.5225\u001b[0m  0.0707\n",
      "     37        \u001b[36m0.1500\u001b[0m       0.6271        0.5249  0.0712\n",
      "     38        \u001b[36m0.1449\u001b[0m       0.6271        \u001b[35m0.5225\u001b[0m  0.0725\n",
      "     39        \u001b[36m0.1416\u001b[0m       0.6305        \u001b[35m0.5188\u001b[0m  0.0722\n",
      "     40        \u001b[36m0.1342\u001b[0m       0.6305        \u001b[35m0.5177\u001b[0m  0.0721\n",
      "     41        \u001b[36m0.1302\u001b[0m       0.6339        0.5179  0.0732\n",
      "     42        \u001b[36m0.1252\u001b[0m       0.6373        0.5195  0.0716\n",
      "     43        \u001b[36m0.1210\u001b[0m       0.6407        \u001b[35m0.5160\u001b[0m  0.0726\n",
      "     44        \u001b[36m0.1166\u001b[0m       0.6407        \u001b[35m0.5119\u001b[0m  0.0728\n",
      "     45        \u001b[36m0.1141\u001b[0m       0.6441        \u001b[35m0.5114\u001b[0m  0.0716\n",
      "     46        \u001b[36m0.1105\u001b[0m       0.6441        0.5124  0.0709\n",
      "     47        \u001b[36m0.1075\u001b[0m       0.6407        0.5128  0.0714\n",
      "     48        \u001b[36m0.1067\u001b[0m       0.6441        \u001b[35m0.5077\u001b[0m  0.0711\n",
      "     49        \u001b[36m0.1057\u001b[0m       0.6373        0.5081  0.0707\n",
      "     50        \u001b[36m0.0991\u001b[0m       0.6407        0.5089  0.0718\n",
      "0.8157635467980296 0.8212765957446808\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m0.6688\u001b[0m       \u001b[32m0.8456\u001b[0m        \u001b[35m0.6298\u001b[0m  0.0955\n",
      "      2        \u001b[36m0.6204\u001b[0m       \u001b[32m0.8557\u001b[0m        \u001b[35m0.6147\u001b[0m  0.0948\n",
      "      3        \u001b[36m0.5686\u001b[0m       0.7848        0.6155  0.0940\n",
      "      4        \u001b[36m0.5106\u001b[0m       0.6962        0.6337  0.0940\n",
      "      5        \u001b[36m0.4600\u001b[0m       0.6608        0.6539  0.0946\n",
      "      6        \u001b[36m0.4205\u001b[0m       0.5722        0.6741  0.0947\n",
      "      7        \u001b[36m0.3886\u001b[0m       0.4709        0.6938  0.0952\n",
      "      8        \u001b[36m0.3607\u001b[0m       0.3671        0.7078  0.0946\n",
      "      9        \u001b[36m0.3386\u001b[0m       0.3544        0.7181  0.0946\n",
      "     10        \u001b[36m0.3212\u001b[0m       0.3494        0.7339  0.0946\n",
      "     11        \u001b[36m0.3060\u001b[0m       0.3519        0.7424  0.0941\n",
      "     12        \u001b[36m0.2946\u001b[0m       0.3646        0.7551  0.0942\n",
      "     13        \u001b[36m0.2840\u001b[0m       0.3696        0.7656  0.0944\n",
      "     14        \u001b[36m0.2726\u001b[0m       0.3848        0.7743  0.0947\n",
      "     15        \u001b[36m0.2597\u001b[0m       0.4101        0.7823  0.0949\n",
      "     16        \u001b[36m0.2483\u001b[0m       0.4177        0.7964  0.0949\n",
      "     17        \u001b[36m0.2414\u001b[0m       0.4253        0.8014  0.0946\n",
      "     18        \u001b[36m0.2342\u001b[0m       0.4304        0.8152  0.0944\n",
      "     19        \u001b[36m0.2281\u001b[0m       0.4304        0.8282  0.0941\n",
      "     20        \u001b[36m0.2223\u001b[0m       0.4354        0.8338  0.0937\n",
      "     21        \u001b[36m0.2170\u001b[0m       0.4405        0.8463  0.0941\n",
      "     22        \u001b[36m0.2119\u001b[0m       0.4481        0.8576  0.0942\n",
      "     23        \u001b[36m0.2072\u001b[0m       0.4506        0.8692  0.0951\n",
      "     24        \u001b[36m0.2025\u001b[0m       0.4582        0.8704  0.0957\n",
      "     25        \u001b[36m0.1978\u001b[0m       0.4608        0.8873  0.0951\n",
      "     26        \u001b[36m0.1931\u001b[0m       0.4658        0.8853  0.0950\n",
      "     27        \u001b[36m0.1898\u001b[0m       0.4329        0.9169  0.0951\n",
      "     28        \u001b[36m0.1863\u001b[0m       0.4608        0.8996  0.0941\n",
      "     29        \u001b[36m0.1834\u001b[0m       0.4582        0.9255  0.0953\n",
      "     30        \u001b[36m0.1811\u001b[0m       0.4734        0.9098  0.0958\n",
      "     31        \u001b[36m0.1787\u001b[0m       0.4886        0.8996  0.0955\n",
      "     32        \u001b[36m0.1758\u001b[0m       0.4911        0.9005  0.0953\n",
      "     33        \u001b[36m0.1743\u001b[0m       0.4987        0.8969  0.0952\n",
      "     34        \u001b[36m0.1715\u001b[0m       0.4962        0.8973  0.0950\n",
      "     35        \u001b[36m0.1677\u001b[0m       0.4582        0.9511  0.0956\n",
      "     36        \u001b[36m0.1674\u001b[0m       0.4911        0.9131  0.0946\n",
      "     37        \u001b[36m0.1629\u001b[0m       0.5114        0.8722  0.0948\n",
      "     38        \u001b[36m0.1623\u001b[0m       0.5089        0.8893  0.0948\n",
      "     39        \u001b[36m0.1592\u001b[0m       0.4987        0.9103  0.0947\n",
      "     40        \u001b[36m0.1578\u001b[0m       0.4987        0.9126  0.0942\n",
      "     41        \u001b[36m0.1563\u001b[0m       0.5013        0.9183  0.0945\n",
      "     42        \u001b[36m0.1563\u001b[0m       0.5418        0.8912  0.0961\n",
      "     43        \u001b[36m0.1537\u001b[0m       0.5392        0.9022  0.0945\n",
      "     44        \u001b[36m0.1524\u001b[0m       0.5215        0.9475  0.0952\n",
      "     45        \u001b[36m0.1511\u001b[0m       0.5241        0.9465  0.0941\n",
      "     46        \u001b[36m0.1486\u001b[0m       0.5114        0.9723  0.0946\n",
      "     47        \u001b[36m0.1481\u001b[0m       0.5367        0.9579  0.0942\n",
      "     48        \u001b[36m0.1459\u001b[0m       0.5038        1.0045  0.0946\n",
      "     49        0.1476       0.5215        0.9913  0.0948\n",
      "     50        \u001b[36m0.1440\u001b[0m       0.5165        1.0107  0.0952\n",
      "0.9394623233908949 0.8893617021276595\n"
     ]
    }
   ],
   "source": [
    "AUC_ROCs = dict()\n",
    "ACCs = dict()\n",
    "model_name = \"LSTM\"\n",
    "print(model_name)\n",
    "AUC_ROCs[model_name] = 0\n",
    "ACCs[model_name] = 0\n",
    "for train_index, test_index in tscv.split(X_train): # Rolling cross-validation happens inside this loop.\n",
    "    X_train_fold, X_validation_fold = X_train.iloc[train_index[:-lag_of_y], X_train.columns != \"USRECD\"], \\\n",
    "        X_train.iloc[test_index[:-lag_of_y], X_train.columns != \"USRECD\"]\n",
    "    y_train_fold, y_validation_fold = y_train.iloc[train_index[lag_of_y:], y_train.columns == \"USRECD\"], \\\n",
    "        y_train.iloc[test_index[lag_of_y:], y_train.columns == \"USRECD\"]\n",
    "\n",
    "    scalers = dict()\n",
    "    for feature in features:\n",
    "        scalers[feature] = StandardScaler()\n",
    "        scalers[feature].fit(X_train_fold[[feature]])\n",
    "        X_train_fold[feature] = scalers[feature].transform(X_train_fold[[feature]])\n",
    "        X_validation_fold[feature] = scalers[feature].transform(X_validation_fold[[feature]])\n",
    "\n",
    "    X_train_fold, y_train_fold = split_sequences(X_train_fold.to_numpy(), y_train_fold.to_numpy(), n_steps=10)\n",
    "    X_train_fold = X_train_fold.astype(np.float32)\n",
    "    y_train_fold = y_train_fold.astype(np.float32)\n",
    "    X_validation_fold, y_validation_fold = split_sequences(X_validation_fold.to_numpy(), y_validation_fold.to_numpy(), n_steps=10)\n",
    "    X_validation_fold = X_validation_fold.astype(np.float32)\n",
    "    y_validation_fold = y_validation_fold.astype(np.float32)\n",
    "    model = get_model()\n",
    "    model.fit(X_train_fold, y_train_fold)\n",
    "    positive_probs = [p[1] for p in model.predict_proba(X_validation_fold)]\n",
    "    AUC_ROC = metrics.roc_auc_score(y_validation_fold, positive_probs)\n",
    "    AUC_ROCs[model_name] += AUC_ROC\n",
    "    predictions = model.predict(X_validation_fold)\n",
    "    ACC = accuracy_score(y_validation_fold, predictions)\n",
    "    ACCs[model_name] += ACC\n",
    "    print(AUC_ROC, ACC)\n",
    "\n",
    "AUC_ROCs[model_name] /= splits\n",
    "ACCs[model_name] /= splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM\n",
      "AUC ROC: 0.6720388934395481\n",
      "accuracy: 0.6457446808510638\n"
     ]
    }
   ],
   "source": [
    "print(model_name)\n",
    "print(f\"AUC ROC: {AUC_ROCs[model_name]}\")\n",
    "print(f\"accuracy: {ACCs[model_name]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5, 0.8560575769692124)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# random guess\n",
    "total = y_train.shape[0]\n",
    "metrics.roc_auc_score(y_train.USRECD, np.zeros(total)), accuracy_score(y_train.USRECD, np.zeros(total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.copy()\n",
    "X_test = X_test.copy()\n",
    "\n",
    "all_scalers = dict()\n",
    "for feature in features:\n",
    "    all_scalers[feature] = StandardScaler()\n",
    "    all_scalers[feature].fit(X_train[[feature]])\n",
    "    X_train[feature] = all_scalers[feature].transform(X_train[[feature]])\n",
    "    X_test[feature] = all_scalers[feature].transform(X_test[[feature]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4.8]\n",
      "[6.44]\n",
      "[3.61]\n",
      "[5.88]\n",
      "[2.29]\n",
      "[3.06]\n",
      "[4.71]\n",
      "[1.81]\n",
      "[6.78]\n",
      "[3.18]\n",
      "[12.51]\n",
      "[3.96]\n",
      "[9.69]\n",
      "[0.01]\n",
      "[2.96]\n"
     ]
    }
   ],
   "source": [
    "for feature in features:\n",
    "    print(all_scalers[feature].mean_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m0.7726\u001b[0m       \u001b[32m0.1603\u001b[0m        \u001b[35m0.7683\u001b[0m  0.1216\n",
      "      2        \u001b[36m0.6450\u001b[0m       \u001b[32m0.3567\u001b[0m        \u001b[35m0.7467\u001b[0m  0.1188\n",
      "      3        \u001b[36m0.5354\u001b[0m       \u001b[32m0.6192\u001b[0m        \u001b[35m0.7164\u001b[0m  0.1185\n",
      "      4        \u001b[36m0.4259\u001b[0m       \u001b[32m0.7335\u001b[0m        \u001b[35m0.6444\u001b[0m  0.1202\n",
      "      5        \u001b[36m0.3415\u001b[0m       0.7275        \u001b[35m0.5340\u001b[0m  0.1210\n",
      "      6        \u001b[36m0.2870\u001b[0m       0.7234        \u001b[35m0.4826\u001b[0m  0.1211\n",
      "      7        \u001b[36m0.2585\u001b[0m       0.7275        \u001b[35m0.4456\u001b[0m  0.1192\n",
      "      8        \u001b[36m0.2384\u001b[0m       0.7315        \u001b[35m0.4319\u001b[0m  0.1186\n",
      "      9        \u001b[36m0.2234\u001b[0m       0.7275        \u001b[35m0.4155\u001b[0m  0.1191\n",
      "     10        \u001b[36m0.2107\u001b[0m       0.7214        \u001b[35m0.4125\u001b[0m  0.1183\n",
      "     11        \u001b[36m0.1986\u001b[0m       0.7174        \u001b[35m0.4119\u001b[0m  0.1184\n",
      "     12        \u001b[36m0.1897\u001b[0m       0.7154        0.4121  0.1185\n",
      "     13        \u001b[36m0.1824\u001b[0m       0.7134        0.4121  0.1179\n",
      "     14        \u001b[36m0.1755\u001b[0m       0.7335        \u001b[35m0.4082\u001b[0m  0.1183\n",
      "     15        \u001b[36m0.1683\u001b[0m       0.7315        0.4096  0.1183\n",
      "     16        \u001b[36m0.1622\u001b[0m       \u001b[32m0.7415\u001b[0m        0.4112  0.1196\n",
      "     17        \u001b[36m0.1584\u001b[0m       \u001b[32m0.7515\u001b[0m        \u001b[35m0.4070\u001b[0m  0.1179\n",
      "     18        \u001b[36m0.1538\u001b[0m       0.7495        0.4089  0.1183\n",
      "     19        \u001b[36m0.1494\u001b[0m       \u001b[32m0.7555\u001b[0m        0.4100  0.1186\n",
      "     20        \u001b[36m0.1466\u001b[0m       \u001b[32m0.7756\u001b[0m        \u001b[35m0.3995\u001b[0m  0.1184\n",
      "     21        \u001b[36m0.1408\u001b[0m       \u001b[32m0.7776\u001b[0m        0.4032  0.1198\n",
      "     22        \u001b[36m0.1388\u001b[0m       \u001b[32m0.7796\u001b[0m        0.4035  0.1212\n",
      "     23        \u001b[36m0.1348\u001b[0m       0.7796        0.4024  0.1182\n",
      "     24        \u001b[36m0.1326\u001b[0m       0.7756        0.4116  0.1190\n",
      "     25        \u001b[36m0.1302\u001b[0m       0.7776        0.4109  0.1178\n",
      "     26        \u001b[36m0.1263\u001b[0m       0.7776        0.4151  0.1196\n",
      "     27        \u001b[36m0.1245\u001b[0m       0.7796        0.4192  0.1190\n",
      "     28        0.1247       \u001b[32m0.7836\u001b[0m        0.4182  0.1188\n",
      "     29        \u001b[36m0.1198\u001b[0m       0.7836        0.4230  0.1186\n",
      "     30        \u001b[36m0.1190\u001b[0m       0.7816        0.4254  0.1193\n",
      "     31        \u001b[36m0.1171\u001b[0m       0.7836        0.4293  0.1194\n",
      "     32        \u001b[36m0.1147\u001b[0m       0.7836        0.4303  0.1194\n",
      "     33        \u001b[36m0.1137\u001b[0m       0.7836        0.4343  0.1190\n",
      "     34        0.1148       \u001b[32m0.7856\u001b[0m        0.4355  0.1192\n",
      "     35        \u001b[36m0.1108\u001b[0m       0.7856        0.4391  0.1197\n",
      "     36        \u001b[36m0.1102\u001b[0m       0.7836        0.4398  0.1190\n",
      "     37        \u001b[36m0.1081\u001b[0m       0.7816        0.4466  0.1196\n",
      "     38        \u001b[36m0.1078\u001b[0m       0.7816        0.4488  0.1195\n",
      "     39        \u001b[36m0.1063\u001b[0m       0.7816        0.4494  0.1208\n",
      "     40        \u001b[36m0.1045\u001b[0m       0.7836        0.4506  0.1196\n",
      "     41        \u001b[36m0.1026\u001b[0m       0.7836        0.4526  0.1187\n",
      "     42        \u001b[36m0.1003\u001b[0m       0.7776        0.4626  0.1193\n",
      "     43        \u001b[36m0.0993\u001b[0m       0.7816        0.4598  0.1196\n",
      "     44        0.0993       0.7836        0.4622  0.1195\n",
      "     45        \u001b[36m0.0981\u001b[0m       0.7776        0.4660  0.1200\n",
      "     46        \u001b[36m0.0971\u001b[0m       0.7655        0.4764  0.1202\n",
      "     47        \u001b[36m0.0960\u001b[0m       0.7816        0.4699  0.1210\n",
      "     48        \u001b[36m0.0945\u001b[0m       0.7756        0.4699  0.1194\n",
      "     49        0.0946       0.7856        0.4715  0.1194\n",
      "     50        \u001b[36m0.0919\u001b[0m       0.7695        0.4815  0.1187\n",
      "0.8952830188679246\n",
      "0.6468401486988847\n"
     ]
    }
   ],
   "source": [
    "print(model_name)\n",
    "model = get_model()\n",
    "\n",
    "X_train, y_train = split_sequences(X_train.to_numpy(), y_train.to_numpy(), n_steps=10)\n",
    "X_train = X_train.astype(np.float32)\n",
    "y_train = y_train.astype(np.float32)\n",
    "\n",
    "X_test, y_test = split_sequences(X_test.to_numpy(), y_test.to_numpy(), n_steps=10)\n",
    "X_test = X_test.astype(np.float32)\n",
    "y_test = y_test.astype(np.float32)\n",
    "    \n",
    "model.fit(X_train, y_train)\n",
    "positive_probs = [p[1] for p in model.predict_proba(X_test)]\n",
    "AUC_ROC = metrics.roc_auc_score(y_test, positive_probs)\n",
    "print(AUC_ROC)\n",
    "predictions = model.predict(X_test)\n",
    "ACC = accuracy_score(y_test, predictions)\n",
    "print(ACC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
